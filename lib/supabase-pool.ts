import postgres from 'postgres';
import { log } from './logger';
import { uploadToOSS } from '@/lib/oss-client';  // ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

// è¿æ¥æ± é…ç½®
const POOL_SIZE = 50; // è¿æ¥æ± å¤§å°
const IDLE_TIMEOUT = 30; // ç©ºé—²è¶…æ—¶ï¼ˆç§’ï¼‰

let sql: postgres.Sql | null = null;
let initializationError: Error | null = null;

// åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ æ¥å£å®šä¹‰
interface ContentBlock {
  parent_id: string;
  block_type: string;
  content: string;
  order_index: number;
  metadata: string;
}

function getRandomString() {
  return Math.random().toString(36).substring(2, 8);
}

/**
 * è·å–æ•°æ®åº“è¿æ¥æ± å®ä¾‹
 */
function getPool(requestId: string = getRandomString()) {
  if (initializationError) {
    log('ERROR', requestId, 'ğŸ”´ è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è·å–è¿æ¥', initializationError);
    throw initializationError;
  }

  if (!sql) {
    try {
      log('INFO', requestId, 'ğŸ”„ åˆå§‹åŒ– PostgreSQL è¿æ¥æ± ');
      
      // æ£€æŸ¥ç¯å¢ƒå˜é‡
      if (!process.env.DATABASE_URL) {
        throw new Error('DATABASE_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®');
      }

      // åˆ›å»ºè¿æ¥æ± ï¼Œä½¿ç”¨æ›´æ–°çš„é…ç½®
      sql = postgres(process.env.DATABASE_URL, {
        max: POOL_SIZE,               // æœ€å¤§è¿æ¥æ•°
        idle_timeout: IDLE_TIMEOUT,   // ç©ºé—²è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_lifetime: 60 * 30,        // è¿æ¥æœ€å¤§ç”Ÿå‘½å‘¨æœŸï¼ˆç§’ï¼‰
        connect_timeout: 10,          // è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰
        prepare: false,               // å¯¹äº‹åŠ¡æ¨¡å¼ç¦ç”¨é¢„å¤„ç†è¯­å¥
        debug: process.env.NODE_ENV === 'development', // å¼€å‘ç¯å¢ƒå¼€å¯è°ƒè¯•
      });
      
      log('INFO', requestId, `âœ… è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸï¼Œå¤§å°: ${POOL_SIZE}`);
    } catch (error: any) {
      log('ERROR', requestId, 'ğŸ”´ è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥', error);
      initializationError = error;
      throw error;
    }
  }

  return sql;
}

/**
 * æ‰§è¡Œå¸¦æœ‰é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æ“ä½œ
 */
export async function executeWithRetry<T>(
  requestId: string,
  operation: string,
  callback: (sql: postgres.Sql) => Promise<T>,
  maxRetries = 3
): Promise<T> {
  let lastError: Error | null = null;
  let retryCount = 0;
  
  while (retryCount < maxRetries) {
    try {
      const pool = getPool(requestId);
      log('DEBUG', requestId, `â±ï¸ æ‰§è¡Œæ•°æ®åº“æ“ä½œ: ${operation} (å°è¯• ${retryCount + 1})`);
      const result = await callback(pool);
      return result;
    } catch (error: any) {
      lastError = error;
      retryCount++;
      
      // å¦‚æœæ˜¯è¿æ¥é—®é¢˜ï¼Œåˆ™ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
      if (error.code === '57P01' || error.code === '57P02' || error.code === '57P03') {
        const delay = Math.min(100 * Math.pow(2, retryCount), 2000); // æŒ‡æ•°å›é€€
        log('WARN', requestId, `âš ï¸ æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œ${retryCount < maxRetries ? `ç­‰å¾… ${delay}ms åé‡è¯•` : 'è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°'}`, error);
        
        if (retryCount < maxRetries) {
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      } else {
        // éè¿æ¥é—®é¢˜ç›´æ¥æŠ›å‡º
        throw error;
      }
    }
  }
  
  throw lastError || new Error(`æ•°æ®åº“æ“ä½œå¤±è´¥: ${operation}`);
}

// æ‰¹é‡æ’å…¥åŠ©æ‰‹å‡½æ•°
export async function batchInsert<T extends Record<string, any>>(
  requestId: string,
  tableName: string,
  records: T[],
  batchSize = 100
): Promise<void> {
  if (!records.length) return;
  
  const sql = getPool(requestId);
  const batches = [];
  
  // å°†è®°å½•åˆ†æ‰¹
  for (let i = 0; i < records.length; i += batchSize) {
    batches.push(records.slice(i, i + batchSize));
  }
  
  log('DEBUG', requestId, `ğŸ”„ æ‰¹é‡æ’å…¥ ${records.length} æ¡è®°å½•åˆ° ${tableName}ï¼Œåˆ† ${batches.length} æ‰¹æ‰§è¡Œ`);
  
  // å¹¶å‘æ‰§è¡Œæ‰¹é‡æ’å…¥ï¼Œä½†é™åˆ¶å¹¶å‘æ•°
  const concurrencyLimit = 5;
  for (let i = 0; i < batches.length; i += concurrencyLimit) {
    const batchPromises = batches.slice(i, i + concurrencyLimit).map(async (batch, idx) => {
      const batchNumber = i + idx + 1;
      log('DEBUG', requestId, `â±ï¸ æ‰§è¡Œæ‰¹æ¬¡ ${batchNumber}/${batches.length}ï¼Œè®°å½•æ•°: ${batch.length}`);
      
      if (batch.length > 0) {
        // è·å–åˆ—å
        const columns = Object.keys(batch[0]);
        
        // æ„å»º SQL æŸ¥è¯¢ï¼Œä½¿ç”¨ VALUES è¯­æ³•
        let query = `INSERT INTO "${tableName}" (${columns.map(c => `"${c}"`).join(', ')}) VALUES `;
        
        // ä¸ºæ¯æ¡è®°å½•åˆ›å»ºå ä½ç¬¦
        const placeholders: string[] = [];
        const values: any[] = [];
        
        batch.forEach((record, recordIndex) => {
          // ä¸ºæ¯æ¡è®°å½•åˆ›å»ºä¸€ç»„å ä½ç¬¦ï¼Œå¦‚ ($1, $2, $3)
          const recordPlaceholders: string[] = [];
          
          columns.forEach(column => {
            recordPlaceholders.push(`$${values.length + 1}`);
            values.push(record[column]);
          });
          
          placeholders.push(`(${recordPlaceholders.join(', ')})`);
        });
        
        query += placeholders.join(', ');
        
        // æ‰§è¡ŒæŸ¥è¯¢
        await sql.unsafe(query, ...values);
      }
    });
    
    await Promise.all(batchPromises);
  }
  
  log('INFO', requestId, `âœ… å®Œæˆæ‰¹é‡æ’å…¥åˆ° ${tableName}ï¼Œå…± ${records.length} æ¡è®°å½•`);
}

/**
 * å…³é—­è¿æ¥æ± 
 */
export async function closePool(): Promise<void> {
  if (sql) {
    await sql.end();
    sql = null;
  }
}

/**
 * é’ˆå¯¹ç« èŠ‚ä¸Šä¼ ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†å‡½æ•°
 */
async function batchProcessChapters(
  requestId: string,
  bookId: string,
  userId: string,
  chapters: Array<{title: string, content: string, blocks: any[]}>
): Promise<{chapterIds: string[], blockCount: number}> {
  const sql = getPool(requestId);
  
  // ä½¿ç”¨å•ä¸ªäº‹åŠ¡å¤„ç†æ‰€æœ‰æ“ä½œ
  return await sql.begin(async (transaction) => {
    log('INFO', requestId, `ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† ${chapters.length} ä¸ªç« èŠ‚ï¼ˆäº‹åŠ¡æ¨¡å¼ï¼‰`);
    
    // 1. åˆ›å»ºæ‰€æœ‰ç« èŠ‚çˆ¶è®°å½• - ä¸€æ¬¡æ€§æ“ä½œ
    const contentParentQuery = `
      INSERT INTO content_parents 
        (content_type, title, user_id, metadata)
      VALUES (
        'chapter',
        unnest($1::text[]),
        $2::uuid,
        jsonb_build_object('book_id', $3::uuid, 'chapter_index', generate_subscripts($1::text[], 1) - 1)
      )
      RETURNING id, title, metadata, (array_position($1::text[], title) - 1) as idx
    `;
    
    const titles = chapters.map(c => c.title);
    const parentInsertResult = await transaction.unsafe<Array<{
      id: string;
      title: string;
      metadata: any;
      idx: number;
    }>>(contentParentQuery, [titles, userId, bookId]);
    
    log('INFO', requestId, `âœ… æ‰¹é‡åˆ›å»ºç« èŠ‚çˆ¶è®°å½•å®Œæˆï¼Œæ•°é‡: ${parentInsertResult.length}`);
    
    // 2. åˆ›å»ºæ‰€æœ‰ç« èŠ‚è®°å½• - ä¸€æ¬¡æ€§æ“ä½œ
    const chaptersQuery = `
      INSERT INTO chapters 
        (book_id, title, order_index, parent_id)
      SELECT 
        $1::uuid as book_id,
        unnest($2::text[]) as title,
        unnest($3::int[]) as order_index,
        unnest($4::uuid[]) as parent_id
      RETURNING id, book_id, title, order_index, parent_id
    `;
    
    // å‡†å¤‡æ•°æ®æ•°ç»„
    const chapterTitles = parentInsertResult.map(p => p.title);
    const chapterOrders = parentInsertResult.map(p => p.idx);
    const parentIds = parentInsertResult.map(p => p.id);
    
    const chapterInsertResult = await transaction.unsafe<Array<{
      id: string;
      book_id: string;
      title: string;
      order_index: number;
      parent_id: string;
    }>>(chaptersQuery, [bookId, chapterTitles, chapterOrders, parentIds]);
    
    log('INFO', requestId, `âœ… æ‰¹é‡åˆ›å»ºç« èŠ‚è®°å½•å®Œæˆï¼Œæ•°é‡: ${chapterInsertResult.length}`);
    
    // 3. åˆ›å»ºæ‰€æœ‰å†…å®¹å— - æ‰¹é‡å¤„ç†
    const allBlocks: ContentBlock[] = [];
    
    // é¢„å¤„ç†æ‰€æœ‰å—
    for (let i = 0; i < chapters.length; i++) {
      const parent = parentInsertResult.find(p => p.idx === i);
      if (!parent) continue;
      
      chapters[i].blocks.forEach((block, blockIndex) => {
        allBlocks.push({
          parent_id: parent.id,
          block_type: block.type,
          content: block.content,
          order_index: blockIndex,
          metadata: JSON.stringify(block.metadata || {})
        });
      });
    }
    
    // ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
    const BATCH_SIZE = 5000;

    // å¹¶è¡Œæ‰§è¡Œæ‰¹é‡æ’å…¥
    const CONCURRENT_INSERTS = 3;
    const batches = [];
    for (let i = 0; i < allBlocks.length; i += BATCH_SIZE) {
      batches.push(allBlocks.slice(i, i + BATCH_SIZE));
    }

    // å¹¶è¡Œæ‰§è¡Œæ‰¹æ¬¡ï¼Œä½†æ§åˆ¶å¹¶å‘æ•°
    for (let i = 0; i < batches.length; i += CONCURRENT_INSERTS) {
      const batchPromises = batches.slice(i, i + CONCURRENT_INSERTS).map(batch => {
        return insertBatch(transaction, batch);
      });
      await Promise.all(batchPromises);
    }
    
    log('INFO', requestId, `âœ… æ‰¹é‡åˆ›å»ºå†…å®¹å—å®Œæˆï¼Œæ€»æ•°: ${allBlocks.length}`);
    
    return {
      chapterIds: chapterInsertResult.map(c => c.id),
      blockCount: allBlocks.length
    };
  });
}

// insertBatch å‡½æ•°ç°åœ¨å¯ä»¥ä½¿ç”¨ ContentBlock æ¥å£
async function insertBatch(transaction: postgres.Sql, batch: ContentBlock[]): Promise<void> {
  if (batch.length === 0) return;
  
  const columns = ['parent_id', 'block_type', 'content', 'order_index', 'metadata'] as const;
  let query = `INSERT INTO context_blocks (${columns.join(',')}) VALUES `;
  
  const placeholders: string[] = [];
  const values: any[] = [];
  
  batch.forEach(block => {
    const rowPlaceholders: string[] = [];
    
    columns.forEach(col => {
      rowPlaceholders.push(`$${values.length + 1}`);
      values.push(block[col]);
    });
    
    placeholders.push(`(${rowPlaceholders.join(',')})`);
  });
  
  query += placeholders.join(',');
  
  // æ‰§è¡ŒæŸ¥è¯¢
  await transaction.unsafe(query, values);
}

// ä¼˜åŒ–èµ„æºä¸Šä¼ å‡½æ•°
async function uploadResourceBatch(
  resources: Array<{data: Buffer, name: string}>,
  concurrency = 5
): Promise<Array<{url: string, name: string}>> {
  const results = [];
  const queue = [...resources];
  
  while (queue.length > 0) {
    const batch = queue.splice(0, concurrency);
    const uploadPromises = batch.map(resource => 
      uploadToOSS(resource.data, resource.name)
    );
    
    const batchResults = await Promise.all(uploadPromises);
    results.push(...batchResults);
  }
  
  return results;
}

// ä¼˜åŒ–æ•°æ®åº“æ‰¹é‡æ’å…¥
async function batchInsertResources(
  transaction: postgres.Sql,
  resources: Array<{url: string, type: string, /* ... */}>,
  batchSize = 1000
) {
  const batches = [];
  for (let i = 0; i < resources.length; i += batchSize) {
    batches.push(resources.slice(i, i + batchSize));
  }
  
  // ä¸²è¡Œæ‰§è¡Œæ‰¹é‡æ’å…¥ï¼ˆå› ä¸ºåœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­ï¼‰
  for (const batch of batches) {
    await transaction.unsafe(buildBatchInsertQuery('book_resources', batch));
  }
}

// æ·»åŠ æ‰¹é‡æ’å…¥æŸ¥è¯¢æ„å»ºå‡½æ•°
function buildBatchInsertQuery(table: string, records: Record<string, any>[]): string {
  if (!records.length) return '';
  
  const columns = Object.keys(records[0]);
  const values = records.map(r => 
    `(${columns.map(c => typeof r[c] === 'string' ? `'${r[c]}'` : r[c]).join(',')})`
  );
  
  return `INSERT INTO ${table} (${columns.join(',')}) VALUES ${values.join(',')}`;
}

// åœ¨åº•éƒ¨ç»Ÿä¸€å¯¼å‡ºæ‰€æœ‰å‡½æ•°
export {
  getPool,
  uploadResourceBatch,
  batchInsertResources,
  buildBatchInsertQuery,
  batchProcessChapters
};

